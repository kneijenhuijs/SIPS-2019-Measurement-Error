---
title: "Measurement error in psychological science"
author: "Koen Neijenhuijs & Sam Parsons"
date: "July .., 2019"
output: 
  ioslides_presentation:
    logo: Pictures/logo2.png
    transition: faster
bibliography: EM.bib
---

## Who we are
<div class="columns-2">
![](Pictures/Koen2.jpg)

Koen Neijenhuijs

\@KNeijenhuijs

k.i.neijenhuijs@vu.nl
</div>

## Measurement Error {.build}
Always there

Always watching

<div class="centered">
![](gifs/underestimate.gif)
</div>

## What is Measurement Error? {.flexbox .vcenter}

**Observational error** (or **measurement error**) is the difference between a measured value of a quantity and its true value.

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
library(ggplot2)
library(ggthemes)

datPlot <- data.frame(Person = c(1, 1, 2, 2), 
                    Value = as.factor(rep(c("Measured", "True"), times=2)),
                    y = c(20, 30, 25, 10))

ggplot(datPlot) + 
  geom_point(aes(x=Person, y=y, color=Value)) + 
  scale_y_continuous(limits=c(0,50)) +
  geom_segment(aes(x=1, xend=1, y=21, yend=29), color="gray55", arrow=arrow()) +
  annotate("text", x=1.2, y=25, label="+10", color="gray55") +
  geom_segment(aes(x=2, xend=2, y=24, yend=11), color="gray55", arrow=arrow()) +
  annotate("text", x=1.8, y=17.5, label="-15", color="gray55") +
  scale_x_continuous(limits=c(0,3), breaks=c(1,2), labels=c("Person 1", "Person 2")) +
  theme_few()
```

## What is Measurement Error?
Systematic measurement error: inaccuracies inherent to the measurement tool

*E.g. Miscalibrated computer for reaction time*

Random measurement error: Unpredictable fluctuations in measurement

*E.g. During reaction time measurement, a participant may be distracted on some trials*

## Determining Measurement Error?
We don't know the true value, so can't calculate...

Can estimate the minimum value change that is not measurement error: Smallest Detectable Change

## Smallest Detectable Change {.build}

$$Reliability = {\sigma^2 participants \over \sigma² participants + \sigma² error}$$

$$Standard\, Error\, of\, Measurement = \sqrt {\sigma² error}$$

$$Standard\, Error\, of\, Measurement = SD * \sqrt {1-ICC}$$

$$Smallest\, Detectable\, Change = 1.96 * \sqrt {2} * SEM$$

$$Smallest\, Detectable\, Change = 1.96 * SD_{change}$$

## Smallest Detectable Change {.build}
$$Smallest\, Detectable\, Change = 1.96 * \sqrt {2} * SEM$$

$$Smallest\, Detectable\, Change = 1.96 * SD_{change}$$

SDC is the smallest change in score that you can detect above the measurement error

Note that we are using a z-distribution (1.96):

5% of cases where we judge the change score above the measurement error, this is a type I error...

<div class="centered">
![](gifs\fozzie.gif)
</div>

## SDC and reliability
SDC can be calculated from ICC

"Reliability is "the degree to which the measurement is free from measurement error" [@Mokkink2010a]

## Limits of Agreement
SDC is not the only value of measurement error

Limits of Agreement show... well, the limits of agreement between two measures

Often used to compare two sets of measurements from two instruments, but can also be used for two sets of measurements from the same instrument

Bland-Altman method:

$$Limits\, of\, Agreement = Mean_{difference} \pm 1.96 * SD_{difference}$$

Limits of Agreement is a Confidence Interval in disguise

## Bland-Altman plot
The plot below is of two measurements with a *Mean* of **20**, and *sd*'s of **2.5** and **10**

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
library(ggplot2)
library(dplyr)
library(ggthemes)

datPlot <- data.frame(Measurement1 = rnorm(40, 20, 2.5),
                      Measurement2 = rnorm(40, 20, 10)) %>% rowwise() %>%
  mutate(Avg = mean(c(Measurement1, Measurement2)), Dif=Measurement1-Measurement2)

ggplot(datPlot, aes(x=Avg, y=Dif)) + 
  geom_point(alpha=0.7) +
  geom_hline(yintercept = mean(datPlot$Dif), colour = "blue", size = 0.5) +
  geom_hline(yintercept = mean(datPlot$Dif) - (1.96 * sd(datPlot$Dif)), colour = "red", size = 0.5) +
  geom_hline(yintercept = mean(datPlot$Dif) + (1.96 * sd(datPlot$Dif)), colour = "red", size = 0.5) +
  ylab("Diff. Between Measures") +
  xlab("Average Measure") +
  theme_few()
```

## How we currently deal with measurement error
We aggregate within participants (to eliminate random measurement error)

*E.g. The different levels of distraction of a participant should even out across trials*

We aggregate between participants (to eliminate systematic measurement error)

*E.g. Participants that don't take the measurement seriously, should be evened out by participants who do*

## Problem solved then?! {.flexbox .vcenter}

![](Pictures/yesbutno.jpg)

## We all love simulations, amirite?

Simulation based on a RCT for treatment of depression, measured by the BDI-II.

BDI-II has range of 0 - 63. Pooled baseline means and sd's of clinical samples from systematic review [@Wang2013]: Baseline mean = 24.1, baseline sd = 11.4

Assume a score decrease of 15.7% for untreated depression [@Posternak2001], which translates to a mean decrease of 9.828. The SD was arbitrarily set to 40% of this decrease to 3.9312.

## Simulation continued

The treatment group received further decrease, based on the MCID of the BDI-II, which was determined to be 18% [@Button2015], leading to a MCID of 4.338.

Measurement error was added based on a percentage of the range of the BDI-II.

## Simulation parameters

Sample size: 250 / 500 / 750

Effect size: 0 / 2\*MCID / 4\*MCID / 6*MCID

Measurement error: 0 / 10% / 20% / 30% / 40%

5000 replications per parameter combination

Simulations performed using SimDesign package [@SimDesign].

Code can be found on my Github: URL here.

## Simulation results

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
library(here)
library(ggplot2)
library(ggthemes)
library(tidyverse)
results <- readRDS(here("results.rds"))
```

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
ggplot(results, aes(x=measError, y=SE_Int, fill=effectSize)) + 
  geom_violin(scale="width", color="black", position = position_dodge(width = 0.9)) + 
  geom_boxplot(color="black", position = position_dodge(width = 0.9), show.legend = FALSE, width=0.3) +
  labs(x="Measurement error", y="SE of Interaction") + 
  scale_x_discrete(labels=c("0%", "10%", "20%", "30%", "40%")) + 
  scale_fill_viridis_d(name = "Effect size", labels = c("0 * MCID", "1 * MCID", "2 * MCID", "3 * MCID")) +
  ggtitle("Standard Error of Interaction Coefficient") +
  theme_gdocs()
```

## Simulation results

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
ggplot(results, aes(x=measError, y=bias_Int)) + 
  geom_violin(scale="width", color="black", position = position_dodge(width = 0.9), fill="deepskyblue4") + 
  geom_boxplot(color="black", position = position_dodge(width = 0.9), show.legend = FALSE, width=0.3, fill="deepskyblue4") +
  labs(color="Mean effect", x="Measurement error", y="Bias of Interaction") + 
  scale_x_discrete(labels=c("0%", "10%", "20%", "30%", "40%")) + 
  ggtitle("Relative Bias of Interaction Coefficient") +
  theme_gdocs()
```

## Simulation results

```{r echo=FALSE, warning=FALSE, message=FALSE, error=FALSE}
ggplot(results, aes(x=measError, y=ETA_Int, fill=effectSize)) + 
  geom_violin(scale="width", color="black", position = position_dodge(width = 0.9)) + 
  geom_boxplot(color="black", position = position_dodge(width = 0.9), show.legend = FALSE, width=0.3) +
  labs(x="Measurement error", y="Eta² of Interaction") + 
  scale_x_discrete(labels=c("0%", "10%", "20%", "30%", "40%")) + 
  scale_fill_viridis_d(name = "Effect size", labels = c("0 * MCID", "1 * MCID", "2 * MCID", "3 * MCID")) +
  ggtitle("ETA² of Interaction Coefficient") +
  theme_gdocs()
```

## Simulation wrap-up

For a more thorough write-up of the simulations, see the Github page for this unconference:

**URL HERE**

## That's enough of us talking

The rest of all slides are of questions and statements for the room to discuss

For some of them we have prepared some answers we could think of

But first, let's take some time for an open discussion

## Is measurement error prevalent in your studies?

<br>
<br>
<br>
<br>
<br>
<br>

<font size="3">
Systematic measurement error: inaccuracies inherent to the measurement tool

*E.g. Miscalibrated computer for reaction time*

Random measurement error: Unpredictable fluctuations in measurement

*E.g. During reaction time measurement, a participant may be distracted on some trials*
</font>

## What about mixed-effect models?

Mixed-effect models can:

- Model the **random variance** inherent to participants across multiple measures
- Model the **random variance** inherent to stimuli across multiple measures
- Model the **random variance** inherent to *pretty much anything* across multiple measures

<font size="3">
Systematic measurement error: inaccuracies inherent to the measurement tool

*E.g. Miscalibrated computer for reaction time*

Random measurement error: Unpredictable fluctuations in measurement

*E.g. During reaction time measurement, a participant may be distracted on some trials*
</font>

## Ideas for questions

- How to measure measurement error in experimental psychology?
- Would it be feasible to set up studies to investigate the measurement error of your measurement tools in your field?
- The way we calculate measurement error is always based on change scores (SDC & LoA). How limiting is this in its' application?
Are the limitations to how we investigate measurement error (SDC & LoA) detrimental to our understanding?
- How can we incorporate measurement error into our data-analyses?

## Ideas for statements

- Measurement error is unmeasurable, and therefore only theoretically relevant

## References

<font size="3">